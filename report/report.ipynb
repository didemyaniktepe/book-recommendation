{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "Hacettepe University \n",
    "Computer Science\n",
    "\n",
    "\n",
    "BBM 409: Introduction to Machine Learning Lab.\n",
    "\n",
    "Student Name Surname: Didem Yanıktepe\n",
    "\n",
    "Student ID: 21527563\n",
    "\n",
    "Instructor: Aykut Erdem\n",
    "\n",
    "TA: Necva Bolucu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Theory Questions\n",
    "\n",
    "### k-Nearest Neighbor Classification\n",
    "\n",
    "1) \n",
    "\n",
    "    a. Given 1-NN(S1) and 1-NN(S2) the label of point x is positive. We find 1-nn with this formula.\n",
    "<img src=\"1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "    So in 1-NN(S1) the minumum distance is posivite and 1-NN(S2) is positive too. If we union them we have 2  positive labels and one of them is closer. So x is positive in our sample.\n",
    "\n",
    "    b. We have 3 neighbors for each sample. In 3-NN(S1) = {+,-,+} 3-NN = {+,+,-} by the way negative ones are closer.So if we union them we have 3-NN(S1US2) = {-,-,+,+,+,+}. If we chose the closest three point our sample will be 3-NN(S3) = {-,-,+}. Then it will be negative.\n",
    "\n",
    "\n",
    "2)\n",
    "\n",
    "    a. A point can be its own neighbor. So, k = 0 minimizes the training set error. The error is 0.\n",
    "    b. k = 5 or k = 7 minimizes the leave-one-out cross-validation error. The error is 4/14.\n",
    "    c.Too big k (k = 13) misclassifies every datapoint (using leave one out cross validation). Too small k leads to overfitting.\n",
    "\n",
    "<img src=\"2.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "    1)  y = 23x1 \n",
    "        X = 23x6(adding bias)\n",
    "        θ = 6x1\n",
    "    \n",
    "    2) Since the closed form can be slow, I prefer the Gradient descent. And we'll go out of 200,000-dimensional matrix will be too big. The process will be slower for the closed form.\n",
    "\n",
    "    3) It accelerates the gradient descent, allowing for less repetition to achieve a good solution. Feature scaling accelerates the gradient descent by avoiding additional iterations that are required when one or more properties take much greater values than others.\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART II: Book Recommendation System\n",
    "   \n",
    "        I implemented a nearest neighbor algorithm to recommend books to readers to their ratings. In addition,recommend books to a user based on user-item ratings.KNN algorithm finds sets of similar users based on common book ratings and make predictions using the average rating of top-k nearest neighborsand also implementation as weighted k-NN algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering\n",
    "        User-based algorithm is a simple algorithmic interpretation of the basic premise of collaborative filtering: past ratings and find other users that are similar to the current user and use ratings with other items to predict what the current user will look like. To estimate the preference for an item that my user does not rate, the user-friendly CF looks for other users who deal with my user with items that they both rate. For that item, these users ' ratings are then weighted by my user's ratings and agreement levels to predict my user's rating.\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "        First i read the csv files with pandas library. I use two library for this assignment : numpy and panda(impoting matplotlib for visulate the datas).Here is my reading csv files code. I delete \"Image-Url-S\" etc Exploring each of these datasets one by one and beginning with books dataset, we can see that image URLs columns do not seem to be required for analysis, and hence these can be dropped off. I filtered the \"BX-Users.csv\" on canada and usa users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 6452: expected 8 fields, saw 9\\nSkipping line 43667: expected 8 fields, saw 10\\nSkipping line 51751: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 92038: expected 8 fields, saw 9\\nSkipping line 104319: expected 8 fields, saw 9\\nSkipping line 121768: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 144058: expected 8 fields, saw 9\\nSkipping line 150789: expected 8 fields, saw 9\\nSkipping line 157128: expected 8 fields, saw 9\\nSkipping line 180189: expected 8 fields, saw 9\\nSkipping line 185738: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 209388: expected 8 fields, saw 9\\nSkipping line 220626: expected 8 fields, saw 9\\nSkipping line 227933: expected 8 fields, saw 11\\nSkipping line 228957: expected 8 fields, saw 10\\nSkipping line 245933: expected 8 fields, saw 9\\nSkipping line 251296: expected 8 fields, saw 9\\nSkipping line 259941: expected 8 fields, saw 9\\nSkipping line 261529: expected 8 fields, saw 9\\n'\n",
      "/Users/didemyaniktepe/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "book = pd.read_csv(\"BX-Books.csv\",sep=\";\",error_bad_lines=False,encoding='latin-1')\n",
    "book.columns = [\"ISBN\",\"Book-Title\",\"Book-Author\",\"Year-Of-Publication\",\"Publisher\",\"Image-URL-S\",\"Image-URL-M\",\"Image-URL-L\"]\n",
    "\n",
    "del book[\"Image-URL-S\"]\n",
    "del book[\"Image-URL-M\"]\n",
    "del book[\"Image-URL-L\"]\n",
    "\n",
    "user = pd.read_csv(\"BX-Users.csv\",sep=\";\",error_bad_lines=False,encoding='latin-1')\n",
    "user.columns = (\"UserID\",\"Location\",\"Age\")\n",
    "user = user[user['Location'].str.contains(\"usa|canada\")]\n",
    "\n",
    "\n",
    "ratingforfind = pd.read_csv(\"train-sklearn.csv\",sep=\";\",error_bad_lines=False,encoding='latin-1')\n",
    "ratingforfind.columns = (\"indexes\",\"UserID\",\"ISBN\",\"BookRating\")\n",
    "del ratingforfind[\"indexes\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>BookRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4017</td>\n",
       "      <td>006000438X</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76483</td>\n",
       "      <td>1557100160</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36135</td>\n",
       "      <td>014025448X</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132679</td>\n",
       "      <td>1853260274</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>129099</td>\n",
       "      <td>1556611536</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID        ISBN  BookRating\n",
       "0    4017  006000438X          10\n",
       "1   76483  1557100160           9\n",
       "2   36135  014025448X           8\n",
       "3  132679  1853260274          10\n",
       "4  129099  1556611536           9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingforfind.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rc(\"font\", size=15)\n",
    "ratingforfind.BookRating.value_counts(sort=False).plot(kind='bar')\n",
    "plt.title('Rating Distribution\\n')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Some books are not rated, some users dont rate the book. So i will merge three csv file book and ratingforfind based on \"ISBN\"; user and ratingforfind based on \"UserID\".  First identify other users similar to the current user in terms of their ratings on the same set of books. For example, if you liked all the “Lord of the rings” books, you identify users which also liked those books. If you found those similar users you take their average rating of books the current user has not yet read … So, how did those “Lord of the rings” lovers rate other books? Maybe they rated “The Hobbit” very high. … and recommend those books with the highest average rating to him.Accordingly, “The Hobbit” has a high average rating and might be recommended to you. These three steps can easily be translated into an alogrithm.However, before we can do that we have to restructure our data. For collaborative filtering data are usually structured that each row corresponds to a user and each column corresponds to a book. This could for example look like this, for 3 users and 5 books. Note that not every user rated every book. For example, user 1 only rated book 3, while user 2 rated book 1 and book 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then i made a pivot table which coloums are Books, rows are User ID values are ratings.It takes a long time because of matrix lenght. It is a huge pivot table so taking too much time. I  filled \"-1\" because some users rate some books 0. If i fill with 0 i could not find the user who rated 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating = pd.merge(book,ratingforfind,on=\"ISBN\")\n",
    "rating = pd.merge(rating,user,on=\"UserID\")\n",
    "rating = rating[rating.BookRating != 0]\n",
    "\n",
    "\n",
    "ratingpivot = rating.pivot_table(index='UserID', columns='ISBN', values='BookRating',fill_value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ISBN</th>\n",
       "      <th>000104799X</th>\n",
       "      <th>000647425X</th>\n",
       "      <th>000648199X</th>\n",
       "      <th>000648302X</th>\n",
       "      <th>000649840X</th>\n",
       "      <th>000651202X</th>\n",
       "      <th>000654861X</th>\n",
       "      <th>000671675X</th>\n",
       "      <th>000673765X</th>\n",
       "      <th>000712032X</th>\n",
       "      <th>...</th>\n",
       "      <th>B0000633PU</th>\n",
       "      <th>B00007MF56</th>\n",
       "      <th>B00008RWPV</th>\n",
       "      <th>B0000AA9JB</th>\n",
       "      <th>B0000AZW79</th>\n",
       "      <th>B0000C2W5U</th>\n",
       "      <th>B0000T6KHI</th>\n",
       "      <th>B0001FZGBC</th>\n",
       "      <th>B0001I1KOG</th>\n",
       "      <th>B000234N3A</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>278832</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278843</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278844</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278851</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278854</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 13313 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "ISBN    000104799X  000647425X  000648199X  000648302X  000649840X  \\\n",
       "UserID                                                               \n",
       "278832          -1          -1          -1          -1          -1   \n",
       "278843          -1          -1          -1          -1          -1   \n",
       "278844          -1          -1          -1          -1          -1   \n",
       "278851          -1          -1          -1          -1          -1   \n",
       "278854          -1          -1          -1          -1          -1   \n",
       "\n",
       "ISBN    000651202X  000654861X  000671675X  000673765X  000712032X  \\\n",
       "UserID                                                               \n",
       "278832          -1          -1          -1          -1          -1   \n",
       "278843          -1          -1          -1          -1          -1   \n",
       "278844          -1          -1          -1          -1          -1   \n",
       "278851          -1          -1          -1          -1          -1   \n",
       "278854          -1          -1          -1          -1          -1   \n",
       "\n",
       "ISBN       ...      B0000633PU  B00007MF56  B00008RWPV  B0000AA9JB  \\\n",
       "UserID     ...                                                       \n",
       "278832     ...              -1          -1          -1          -1   \n",
       "278843     ...              -1          -1          -1          -1   \n",
       "278844     ...              -1          -1          -1          -1   \n",
       "278851     ...              -1          -1          -1          -1   \n",
       "278854     ...              -1          -1          -1          -1   \n",
       "\n",
       "ISBN    B0000AZW79  B0000C2W5U  B0000T6KHI  B0001FZGBC  B0001I1KOG  B000234N3A  \n",
       "UserID                                                                          \n",
       "278832          -1          -1          -1          -1          -1          -1  \n",
       "278843          -1          -1          -1          -1          -1          -1  \n",
       "278844          -1          -1          -1          -1          -1          -1  \n",
       "278851          -1          -1          -1          -1          -1          -1  \n",
       "278854          -1          -1          -1          -1          -1          -1  \n",
       "\n",
       "[5 rows x 13313 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingpivot.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Find similar users with similarities\n",
    "\n",
    "     For this step i select users that have in common that they rated the same books. I choose the one user ond one book on train rating set i delete it in Train Data set which is leave one out cross validation firstly.And i took the users who rated same book.For these users, i calculated the similarity of their ratings with me my user ratings. There is a number of options to calculate similarity, adjusted cosine similarity. Typically they are used. Here, I chose pearson’s correlation. I would go through all the selected users and calculate the similarity between their and my user's ratings. I looked the similarities cosine similarity and the others. The best one is pearson’s correlation so i choose it. And i found all neighbours with their similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Similarity : \n",
    "\n",
    "<img src=\"3.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "####  Adjusted Cosine Similarity:\n",
    "<img src=\"4.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "#### Pearson Correlation Similarity:\n",
    "<img src=\"5.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find users who rate the book\n",
    "def findusersratebook(all_rates,user_matrix,location):\n",
    "    users_rated_book_indexs = list()\n",
    "\n",
    "    count = -1\n",
    "    for x in all_rates:\n",
    "        count += 1\n",
    "        if (x[location] != -1):\n",
    "            users_rated_book_indexs.append(count)\n",
    "\n",
    "    dict_index = {}\n",
    "\n",
    "    for i in range(len(users_rated_book_indexs)):\n",
    "        dict_index[user_matrix[users_rated_book_indexs[i]]] = all_rates[users_rated_book_indexs[i]]\n",
    "\n",
    "    return  dict_index\n",
    "\n",
    "\n",
    "def pearsoncorrelation(object1,traindata):\n",
    "    object1 = object1.clip(0)\n",
    "\n",
    "    allvalues = dict()\n",
    "    for key,object2 in traindata.items():\n",
    "        object2 = object2.clip(0)\n",
    "        result1 = np.corrcoef(object1,object2)[0,1]\n",
    "        if(np.isfinite(result1)):\n",
    "            allvalues[result1]=key\n",
    "\n",
    "\n",
    "    mylist = list(allvalues.keys())\n",
    "    myother = sorted(mylist,reverse=True)\n",
    "    knn = dict()\n",
    "    neighbours = list()\n",
    "    k = 0\n",
    "    for i in allvalues:\n",
    "        knn[allvalues[myother[k]]] = myother[k]\n",
    "        neighbours.append(allvalues[myother[k]])\n",
    "        k+=1\n",
    "\n",
    "    return neighbours\n",
    "\n",
    "\n",
    "def cosinesimilarity(a,traindata):\n",
    "\n",
    "    a = a.clip(0)\n",
    "    similarity_key = dict()\n",
    "    for key,b in traindata.items():\n",
    "        b = b.clip(0)\n",
    "        dot_product = np.dot(a, b)\n",
    "        norm_a = np.linalg.norm(a)\n",
    "        norm_b = np.linalg.norm(b)\n",
    "        result1= dot_product / (norm_a * norm_b)\n",
    "        if(result1 != 0):\n",
    "            similarity_key[key]=result1\n",
    "\n",
    "    sorted_neigbours = sorted(similarity_key.items(), key=lambda x: x[1])\n",
    "    neighbours = list()\n",
    "    reversed(sorted_neigbours)\n",
    "\n",
    "    for i,v in sorted_neigbours:\n",
    "        if(np.isfinite(v)):\n",
    "            print(i,v)\n",
    "            neighbours.append(i)\n",
    "    neighbours = reversed(neighbours)\n",
    "    return neighbours\n",
    "\n",
    "\n",
    "def cosinesimilarity1(a,traindata):\n",
    "\n",
    "    a = a.clip(0)\n",
    "    \n",
    "    for key,b in traindata.items():\n",
    "        b = b.clip(0)\n",
    "        first = np.norm(a)\n",
    "        second = np.norm(b)\n",
    "        product = np.dot(a, b)\n",
    "        result1 = product / (first * second)\n",
    "\n",
    "    return result1\n",
    "\n",
    "\n",
    "def adjustedCosineMatrix(a,traindata):\n",
    "\n",
    "    users = traindata.index.size\n",
    "    books = traindata.colums.size\n",
    "\n",
    "    similar = np.zeros((users, books))\n",
    "    avarage = np.array(users)\n",
    "\n",
    "    for i in range(books):\n",
    "        for j in range(i, book):\n",
    "            similar[i][j] = cosinesimilarity1(traindata[:i] - avarage,traindata[:j] - avarage)\n",
    "    return similar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Find k-nn and weighted k-nn \n",
    "    \n",
    "#### K Nearest Neighbor:\n",
    "    \n",
    "    I found similarities on users now i found how many user i should choose. First i started 3 user then i measure the difference between real ratings and predicted ratings then 5,7,9. The best predicted was 3. \n",
    "<img src=\"8.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Some exprement result:\n",
    "    \n",
    "    k=3\n",
    "    knn :  7.0 weight :  6.999134199350537 real prediction:  7\n",
    "    knn :  9.5 weight :  9.49995420127028 real prediction:  8\n",
    "    knn :  8.0 weight :  7.996904034659469 real prediction:  4\n",
    "    knn :  6.5 weight :  6.499080883388205 real prediction:  4\n",
    "    knn :  7.0 weight :  6.996723999653885 real prediction:  9\n",
    "    knn :  7.5 weight :  7.4988095260590955 real prediction:  4\n",
    "    knn :  8.0 weight :  7.995535721700078 real prediction:  7\n",
    "    knn :  5.0 weight :  10.0 real prediction:  10\n",
    "    knn :  8.0 weight :  7.999884366327989 real prediction:  10\n",
    "    knn :  4.5 weight :  9.0 real prediction:  8\n",
    "    knn :  9.5 weight :  9.50041956235842 real prediction:  9\n",
    "    knn :  8.5 weight :  8.49500016666 real prediction:  7\n",
    "    knn :  9.5 weight :  9.496592940386892 real prediction:  10\n",
    "    knn :  4.0 weight :  8.0 real prediction:  9\n",
    "    knn :  8.0 weight :  7.998968067360278 real prediction:  9\n",
    "    \n",
    "    k=5\n",
    "    knn :  7.8 weight :  7.798307603698272 real prediction:  7\n",
    "    knn :  8.6 weight :  8.600388093368313 real prediction:  8\n",
    "    knn :  7.2 weight :  7.194305511070164 real prediction:  4\n",
    "    knn :  7.0 weight :  6.998601590775316 real prediction:  4\n",
    "    knn :  7.4 weight :  7.398006879322595 real prediction:  9\n",
    "    knn :  8.0 weight :  7.9888117638257 real prediction:  4\n",
    "    knn :  8.0 weight :  7.997769231724429 real prediction:  7\n",
    "    knn :  2.0 weight :  10.0 real prediction:  10\n",
    "    knn :  7.4 weight :  7.399805029063916 real prediction:  10\n",
    "    knn :  1.8 weight :  9.0 real prediction:  8\n",
    "    knn :  8.8 weight :  8.797899633747603 real prediction:  9\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(user_and_rates_matrix,neighbourlist,location):\n",
    "\n",
    "    rating_predictionknn = 0\n",
    "    k_number = 0\n",
    "\n",
    "    for i in neighbourlist:\n",
    "        if(k_number<5):\n",
    "            k_number+=1\n",
    "            rating_predictionknn= rating_predictionknn + user_and_rates_matrix.get(i)[location]\n",
    "\n",
    "\n",
    "    return rating_predictionknn/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted K Nearest Neighbor:\n",
    "        Differences between weighted knn and knn : we include the weights and we divide it with weights.We add weight because some users more similar than the other users. So if we added we weight them our prediction will be good. I searched how can i choose the weights. Firstly i choose similarities but unfortunately in pearson correlation can be negative, in cosine similarity can be zero. So i choose the distance because it can not be negative. A decaying exponential of the form e−αx where x x is the distance from the observation) is convenient in this situation. It has the nice feature that the weight is equal to 1 when the observation lies exactly at one of your training points and decays to zero as x→∞ . I choose Manathan Distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightedknn(location,user,neigbours,user_and_rates_matrix):\n",
    "    weighted_rating_predict = 0\n",
    "    a = len(user)\n",
    "    user = user.clip(0)\n",
    "    distance = list()\n",
    "\n",
    "    m = 0\n",
    "    for x in neigbours:\n",
    "        if(m<5):\n",
    "            m+=1\n",
    "            y = user_and_rates_matrix.get(x)\n",
    "            y = y.clip(0)\n",
    "            suma = manathan_distance(user,y)\n",
    "            distance.append(suma)\n",
    "\n",
    "\n",
    "\n",
    "    k_number = 0\n",
    "    total_distance = 0\n",
    "\n",
    "    for i in neigbours:\n",
    "        if (k_number < 5):\n",
    "            weighted_rating_predict = weighted_rating_predict + (math.exp((1/distance[k_number])) * user_and_rates_matrix.get(i)[location])\n",
    "            total_distance = total_distance + (math.exp(1/distance[k_number]))\n",
    "            k_number += 1\n",
    "\n",
    "\n",
    "    return weighted_rating_predict/total_distance\n",
    "\n",
    "\n",
    "def manhattan_distance(first, second):\n",
    "    return sum([abs(first[i]-second[i]) for i in range(len(first))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        The mean absolute error calculates an accuracy . MAE measures the predictions made by knn are compared to actual ratings. If {r1,...,rN} are all the real values in the target set, and {p1,.,pN} are the predicted values for the same ratings, and E = {ε1,...,εN} = {p1 − r1,...,pN − rN} are the errors. The smallest the mean absolute error, the more accurate ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"6.png\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Finally Mean Absolute Error in this part i do cross validation on my dataset. I splited the data (4/5)train and (1/5) validation.I did it with sklearn library and my code.As there is never enough data to train your model, removing a part of it for validation poses a problem of underfitting. By reducing the training data, we risk losing important patterns/ trends in data set, which in turn increases error induced by bias. So, what we require is a method that provides ample data for training the model and also leaves ample data for validation. K Fold cross validation does exactly that. To make it randomly very hard on code for me so i want to see it with this library just made myself sure.In this assignment Handling with huge dataset is hard to me because i waited for the running my code a while so just try i filtered my data then i tried to large datesets. Here is my filtering code and mean absolute code. For mean absoluate code i used numpy library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"7.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = ratingforfind[\"UserID\"].value_counts()\n",
    "ratingforfind = ratingforfind[ratingforfind[\"UserID\"].isin(counts[counts>=5].index)]\n",
    "counts1 = ratingforfind[\"BookRating\"].value_counts()\n",
    "ratingforfind = ratingforfind[ratingforfind[\"BookRating\"].isin(counts1[counts1>=5].index)]\n",
    "#i deleted it after just for see the data clearly.\n",
    "\n",
    "def MAE(real_values, predicted_values):\n",
    "    return np.abs(real_values - predicted_values).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conculision\n",
    "        \n",
    "        A Nearest Neighbor Algoritm is implemented to produce recommendations between users such that similar users who rated the same book were considered.Both Weighted knn and knn were used and evaluations carried out on each.\n",
    "        The first problem is sparsity. In reality, most users do not rate most items hence the probability of finding a set of users with similar ratings is usually low . The second problem is zeros. This is where an item cannot be predicted rating unless a user has rated before. To overcome these potential problems in real-world systems, future work could include research into exploiting content information of items already rated and combining it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
